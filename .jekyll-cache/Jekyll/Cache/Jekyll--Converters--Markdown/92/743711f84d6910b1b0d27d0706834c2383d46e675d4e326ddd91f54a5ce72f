I"ñ<p>Jun Hurâ€™s Blog Post 5 for PIC 16B!</p>

<h1 id="image-classification-in-tensorflow">Image Classification In Tensorflow</h1>

<p>Today we are interested in teaching a machine learning algorithm to distinguish between pictures of dogs and cats.</p>

<p>The complete instruction can be found <a href="https://www.philchodrow.com/PIC16B//posts/blog-post-5"> here </a>.</p>

<h1 id="1load-packages-and-obtain-data">1.Load Packages and Obtain Data</h1>

<p><img src="/images/blog_5_1_0.JPG" alt="_config.yml" /></p>

<p>By running the following code, we create TensorFlow <code class="language-plaintext highlighter-rouge">Dataset</code>s for training, validation, and testing.</p>

<p>Again, from <strong>Blog Post 1</strong> we did the other day, we can use data sets in cases where itâ€™s not practical to load all the data into memory.</p>

<p>Using <code class="language-plaintext highlighter-rouge">image_dataset_from_directory</code> in <code class="language-plaintext highlighter-rouge">keras</code> <code class="language-plaintext highlighter-rouge">utils</code>, we specify that</p>

<ul>
  <li>the images will be located in <code class="language-plaintext highlighter-rouge">train_dir</code> or <code class="language-plaintext highlighter-rouge">validation_dir</code></li>
  <li>the data will be random by <code class="language-plaintext highlighter-rouge">shuffle=True</code></li>
  <li>32 images will be loaded in each of the data sets by <code class="language-plaintext highlighter-rouge">batch_size = BATCH_SIZE</code></li>
  <li>the size of image will be 160 x 160 by <code class="language-plaintext highlighter-rouge">image_size = IMG_SIZE</code></li>
</ul>

<p>Additionally, the following code is for rapid reading:</p>

<p><img src="/images/blog_5_1_1.JPG" alt="_config.yml" /></p>

<h2 id="working-with-datasets">Working with Datasets</h2>

<p>Finally, we can get a piece of a data set using the <code class="language-plaintext highlighter-rouge">take</code> method.</p>

<p>Here, we will <strong>write a function to create a two-row visualization</strong>.
More specifically, the first row shows three random pictures of <strong>cats</strong> and the second row shows three random pictures of <strong>dogs</strong></p>

<p>This will be a hint:</p>
<ul>
  <li>The following code creates an <em>iterator</em> called <code class="language-plaintext highlighter-rouge">labels</code>.
<img src="/images/blog_5_1_2.JPG" alt="_config.yml" /></li>
  <li>And, by referring to this <a href="https://www.tensorflow.org/tutorials/images/transfer_learning"> site, </a> the following code will show <strong>3x3 images and labels from the training set</strong>. 
<img src="/images/blog_5_1_3.JPG" alt="_config.yml" />
<img src="/images/blog_5_1_4.JPG" alt="_config.yml" /></li>
</ul>

<p>To <strong>compute the number of images</strong> in the training data with label <code class="language-plaintext highlighter-rouge">0</code> or <code class="language-plaintext highlighter-rouge">cats</code> and <code class="language-plaintext highlighter-rouge">1</code> or <code class="language-plaintext highlighter-rouge">dogs</code>, we come back to our iterator defined above:</p>

<p><img src="/images/blog_5_1_5.JPG" alt="_config.yml" /></p>

<p>We have 1000 cats and dogs. Therefore, the <em>baseline</em> is <strong>0.5, or 50%</strong></p>

<h1 id="2first-model">2.First Model</h1>

<p>For our first model, we will use <code class="language-plaintext highlighter-rouge">tf.keras.Sequential</code> model using <code class="language-plaintext highlighter-rouge">layers</code> discussed in class, especially <a href="https://colab.research.google.com/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-2.ipynb#scrollTo=infinite-installation"> here </a>.</p>

<p>More specifically, in each model, we will include:</p>

<ul>
  <li>at least <strong>two</strong> <code class="language-plaintext highlighter-rouge">Conv2D</code> layers</li>
  <li>at least <strong>two</strong> <code class="language-plaintext highlighter-rouge">MaxPooling2D</code> layers</li>
  <li>at least <strong>one</strong> <code class="language-plaintext highlighter-rouge">Flatten</code> layer</li>
  <li>at least <strong>one</strong> <code class="language-plaintext highlighter-rouge">Dense</code> layer</li>
</ul>

<p>Lastly, to train our model, we will</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model1</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> 
                     <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                     <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_dataset</span><span class="p">)</span>
</code></pre></div></div>

<p>For our first model, I aim to do a <strong>bit better</strong> than the baseline, so <strong>at least 52% validation accuracy</strong>.</p>

<p><img src="/images/blog_5_2_0.JPG" alt="_config.yml" />
<img src="/images/blog_5_2_1.JPG" alt="_config.yml" /></p>

<h2 id="assessment">Assessment</h2>
<ul>
  <li>First, the accuracy of the first model stabilized <strong>between 52% and 61%</strong> during training. We achieved our goal of at least 52%!</li>
  <li>Considering that the baseline was 50%, this is definitely better.</li>
  <li>However, we see <strong>clear Overfitting</strong>. That is, we have <strong>100%</strong> training accuracy while the validation accuracy is about 60%. We need to come up with a better model.</li>
</ul>

<p>Before we move on, I would like to look at the learning curves of the training and validation accuracy/loss. Refer <a href="https://www.tensorflow.org/tutorials/images/transfer_learning"> here </a> for more information.</p>

<p><img src="/images/blog_5_2_2.JPG" alt="_config.yml" />
<img src="/images/blog_5_2_3.JPG" alt="_config.yml" /></p>

<p>As we expected, severe overfitting.</p>

<h1 id="3-model-with-data-augmentation">3. Model with Data Augmentation</h1>
<p>We are going to add some <em>data augmentation</em> to our model.</p>

<ul>
  <li>
    <p>First, we will create a <code class="language-plaintext highlighter-rouge">tf.keras.layers.RandomFlip()</code> layer. This will show a flipped with <code class="language-plaintext highlighter-rouge">RandomFlip</code>. The documentation is also <a href="https://www.tensorflow.org/tutorials/images/transfer_learning"> here </a>.</p>
  </li>
  <li>
    <p>Next, we will create a <code class="language-plaintext highlighter-rouge">tf.keras.layers.RandomRotation()</code> layer. This will show a rotated with <code class="language-plaintext highlighter-rouge">RandomRotation()</code>. The documentation is also <a href="https://www.tensorflow.org/tutorials/images/transfer_learning"> here </a>.</p>
  </li>
</ul>

<h2 id="flip">Flip</h2>
<p><img src="/images/blog_5_3_0.JPG" alt="_config.yml" /></p>

<h2 id="rotation">Rotation</h2>
<p><img src="/images/blog_5_3_1.JPG" alt="_config.yml" /></p>

<h2 id="enhance-the-model">Enhance the model</h2>

<p><img src="/images/blog_5_3_2.JPG" alt="_config.yml" />
<img src="/images/blog_5_3_3.JPG" alt="_config.yml" /></p>

<p>Here, we added <code class="language-plaintext highlighter-rouge">RandomFlip()</code> and <code class="language-plaintext highlighter-rouge">RandomRotation()</code> layers, and kept almost other layers the same.</p>

<p><strong>NOTE</strong>: I applied 0.05 for dropOut value for optimization purpose.</p>

<p>As a result, we see a similar set of values for validation accuracy, converging to around <strong>60%</strong>. 
<strong>Great</strong>!</p>

<p>Also, we have the training accuracy value drop to around 65% also, which is a great point as well, we have much less overfitting this time.</p>
:ET
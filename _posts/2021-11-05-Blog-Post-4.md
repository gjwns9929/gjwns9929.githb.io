---
layout: post
title: PIC 16B - Blog Post 4
---


Jun Hur's Blog Post 4 for PIC 16B!

# Spectral Clustering

Here, we will write a tutorial on a simple version of the spectral clustering algorithm for clustering data points.

## Notation
In all the math below:

- Boldface capital letters like **A** refer to matrices (2d arrays of numbers).
- Boldface lowercase letters like **v** refer to vectors (1d arrays of numbers).
- **AB** refers to a matrix-matrix product (A@B). **Av** refers to a matrix-vector product (A@v)

## Motivation

Consider the following plot:

![_config.yml]({{ site.baseurl }}/images/blog_4_code_0.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_moti_0.JPG)

They are clearly divided by two *groups*, so such spectral clustering is not needed.

*Clustering* refers to the task of separating this data set into the two natural **"blobs."** K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these:

![_config.yml]({{ site.baseurl }}/images/blog_4_code_1.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_moti_1.JPG)

What about somewhat more complex dataset?

![_config.yml]({{ site.baseurl }}/images/blog_4_code_2.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_moti_2.JPG)

![_config.yml]({{ site.baseurl }}/images/blog_4_code_3.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_moti_3.JPG)

So, the clustering is not as good as before, is it?

So, we will use **spectral clustering** to do better.

## Part A : *Similarity Matrix*  **A**

Here, **A** will be a matrix (2d) with shape `(n,n)`, where each entry `A[i,j]` is equal to 1 if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]`, and `0` otherwise

For this part, we will use `epsilon = 0.4`

Let's recall that the matrix `X` contains the Euclidean **coordinates of the data points** and vector `y` contains the **labels of each point**.

![_config.yml]({{ site.baseurl }}/images/blog_4_A_0.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_A_1.JPG)


To create such matrix **A**, we need to use `sklearn.metrics.pairwise` from `pairwise_distances`. The doc for this is <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html"> here </a>. 

First, we fill in the entries of **A** with `True` or `False`, depending on whether `X` is within distance of epsilon of `X[j]`. Then, we convert the entry values to `0` or `1`.

Finally, we set the entries of **A** to 0, that is, `A[i,i] = 0`.

![_config.yml]({{ site.baseurl }}/images/blog_4_A_2.JPG)


## Part B.1 : The Cut Term

We will call $$d_i = \sum_{j = 1}^n a_{ij}$$ the *degree of i*, so the `ith row-sum of A`. 

Also, let $$C_0$$ and $$C_1$$ be two clusters of the data points, and each data point is in either $$C_0$$ or $$C_1$$.

Now, the binary norm cut object of a matrix **A** is:

$$ N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

Where

- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$  is the cut of the clusters $$C_0$$ and $$C_1$$.
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$ where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row i. Thus, the *volume* of cluster is a measure of the size of the cluster.

First, the cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in **A** that relate points in cluster $$C_0$$ to points in cluster $$C_1$$.

A function `cut(A,y)` will compute the cut term. Here, we will sum up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters.

Let's talk about the `y` variable, which is *label*. More specifically, `y[i]` is **the label of point i**. For clustering purpose, we set 

$$y[i] = 0 $$, if $$i \in C_0$$

$$y[i] = 1 $$, if $$i \in C_1$$

Thus, we will first define $$C_0$$ and $$C_1$$ as **indices** of array `y` in a sense that $$C_0$$ contains the indices of point whose label is equal to 0 and that $$C_1$$ contains the indices of point whose label is equal to 1.

![_config.yml]({{ site.baseurl }}/images/blog_4_B_0.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_B_1.JPG)

Check if two clusters share any point in common:

![_config.yml]({{ site.baseurl }}/images/blog_4_B_2.JPG)

Great!

Now, the function is defined as follows:

![_config.yml]({{ site.baseurl }}/images/blog_4_B_3.JPG)

Let's confirm that the cut objective indeed favors the true clusters over the random ones.

![_config.yml]({{ site.baseurl }}/images/blog_4_B_4.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_B_5.JPG)

## Part B.2 : The Volume Term

Let's look at the `volume term`. 

The **volume** of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is.

We will write a function `vols(A,y)` that computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple.

![_config.yml]({{ site.baseurl }}/images/blog_4_B_6.JPG)

Let's check the volumes!

![_config.yml]({{ site.baseurl }}/images/blog_4_B_7.JPG)

Let's add those two functions to create `normcut(A,y)` to compute the **binary normalized cut object** for **A** and `y`.

![_config.yml]({{ site.baseurl }}/images/blog_4_B_8.JPG)

Now, finally we compare `normcut` on true label `y` and fake labels we created above.

![_config.yml]({{ site.baseurl }}/images/blog_4_B_9.JPG)

Clearly, the norm cut for true label is **significantly smaller** than that for fake label.

## Part C:

We need a math trick as a cluster vector `y` that makes `normcut(A,y)` small is an NP-hard.

That is, we need to come up with a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that 
$$z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}$$

Therefore, if `i` is in cluster $$C_0$$, then $$z_i > 0$$ and vice versa. 

Then, we will show 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$
where **D** is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$

with a function `transform(A,y)` that computes the appropriate **z** vector given `A` and `y`. 

Consider the equation: $$ z = \frac{1}{\mathbf{vol}(C_0)} -(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)})y_i $$.

This will achieve
 $$z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}$$

Therefore, the `transform(A,y)` function is simply:

![_config.yml]({{ site.baseurl }}/images/blog_4_C_0.JPG)

Next, we will check the equations in **Part B** and **Part C** are equal.

![_config.yml]({{ site.baseurl }}/images/blog_4_C_1.JPG)

Then, we will prove that $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

![_config.yml]({{ site.baseurl }}/images/blog_4_C_2.JPG)

## Part D: Optimization : continuous relaxation of the normcut

Recall that t the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 
$$R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}$$

We start with an `orth_obj` function given by professor:

![_config.yml]({{ site.baseurl }}/images/blog_4_D_0.JPG)

We will use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` w.r.t **z**. Refer to the full doc <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"> here </a>.

For our initial guess, we use 1's.

![_config.yml]({{ site.baseurl }}/images/blog_4_D_1.JPG)

Then, according to the doc, `.x` will contain the solution array.

![_config.yml]({{ site.baseurl }}/images/blog_4_D_2.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_D_3.JPG)

## Part E: Plot the original data on z_min
Here, we will plot the original data:
![_config.yml]({{ site.baseurl }}/images/blog_4_code_2.JPG)
![_config.yml]({{ site.baseurl }}/images/blog_4_moti_2.JPG)

while using `z_min[i]` being **negative** or **non-negative**.

![_config.yml]({{ site.baseurl }}/images/blog_4_E_0.JPG)

Now the clustering looks **correct** indeed!

## Part F: Use Eigenvalues and Eigenvectors

We can use **eigenvalues** and **eigenvectors** to solve the problem more efficiently. First, recall our problem to minimize:

$$R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}$$

w.r.t **z** subject to $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

The *Rayleigh-Ritz* Theorem states that the minimizing z must be the solution with smallest eigenvalue of the generalized eigenvalue problem

$$(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

Thus

$$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

**So, the vector z that we want must be the eigenvector with the second-smallest eigenvalue.**

First, we will construct a matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, called *Laplacian* matrix of **A**

![_config.yml]({{ site.baseurl }}/images/blog_4_F_0.JPG)

Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`

**Note**: I referred to <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html"> here for this task.

Thus, `W` contains **eigenvalues** and `V` contains **eigenvectors**.
Then, simply find the eigenvector corresponding to its second-smallest eigenvalue using `np.argsort()` with `index=1`.

![_config.yml]({{ site.baseurl }}/images/blog_4_F_1.JPG)

Let's plot the original data with `z_eig`!

![_config.yml]({{ site.baseurl }}/images/blog_4_F_2.JPG)

Did this plot improve from **Part E**? 

![_config.yml]({{ site.baseurl }}/images/blog_4_E_0.JPG)

**Yes**!

## Part G: `spectral_clustering(X,epsilon)` function
Simply, we will sum up our work in a function called `spectral_clustering(X,epsilon)`


![_config.yml]({{ site.baseurl }}/images/blog_4_G_0.JPG)

We can confirm the plot with the original data using this function:

![_config.yml]({{ site.baseurl }}/images/blog_4_G_1.JPG)

## Part H: Experiment with an increse of noise

Here, we set `n = 1000` thanks to this faster function and experiment with an increase of `noise`. 

**noise = 0.05**

![_config.yml]({{ site.baseurl }}/images/blog_4_H_0.JPG)

**noise = 0.1**

![_config.yml]({{ site.baseurl }}/images/blog_4_H_1.JPG)

**noise = 0.2**

![_config.yml]({{ site.baseurl }}/images/blog_4_H_2.JPG)


**Comment**: Clearly, as `noise` increases, the data points spread out, and the clustering algorithm appears to divide the data in **circle-like clusters, not the two half-moon clusters**.

## Part I : The bull's eye

Consider the bull's eye data points:

![_config.yml]({{ site.baseurl }}/images/blog_4_I_0.JPG)

Clearly, `k-means()` does not work very well.

Let's apply our function to this:

**epsilon = 0.4 (default)**

![_config.yml]({{ site.baseurl }}/images/blog_4_I_1.JPG)

Good!

**epsilon = 0.3**

![_config.yml]({{ site.baseurl }}/images/blog_4_I_2.JPG)

Good!

**epsilon = 0.2**

![_config.yml]({{ site.baseurl }}/images/blog_4_I_3.JPG)

**Not** Good :(

**epsilon = 0.5**

![_config.yml]({{ site.baseurl }}/images/blog_4_I_4.JPG)

Good!

**epsilon = 0.6**

![_config.yml]({{ site.baseurl }}/images/blog_4_I_5.JPG)

**Not** Good :(

**Conclusion**: It appears that the function works well for epsilon values between (roughly) **0.3 and 0.5**

